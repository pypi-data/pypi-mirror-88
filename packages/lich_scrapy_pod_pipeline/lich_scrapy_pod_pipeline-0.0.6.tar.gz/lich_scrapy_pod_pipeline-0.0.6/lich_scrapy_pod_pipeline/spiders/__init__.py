# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.

import scrapy
from os_scrapy_linkextractor.lx_extensions import LxExtensionManager
from os_scrapy_record.items import fetch_record


class ExampleSpider(scrapy.Spider):
    """ ExampleSpider
    Auto generated by os-scrapy-cookiecuter

    Run:
        scrapy crawl example
    """

    name = "example"

    def start_requests(self):
        yield scrapy.Request(
            url="http://www.example.com/",
            meta={
                "pod.addr": "http://127.0.0.1:6789/api/queue/batchEnqueue/",
                "extractor.rules": [
                    {"type": "re", "allow_domains": [], "deny_domains": [],},
                    {
                        "type": "lxml",
                        "allow": [],
                        "deny": [],
                        "allow_domains": [],
                        "deny_domains": [],
                        "restrict_xpaths": [],
                        "tags": ["a", "area"],
                        "attrs": ["href"],
                        "restrict_css": [],
                        "meta": {"named.tag": "2"},
                    },
                ],
                "extractor.links": [
                    ["http://www.sogou.com/"],
                    ["http://www.baidu.com/"],
                ],
                "reserved": {"root_link": "www.baidu.com"},
            },
        )

    def parse(self, response):
        response = self.lx_manager.process_response(response)
        yield fetch_record(response=response)

    def _set_lxmanager(self, crawler):
        self.lx_manager = LxExtensionManager.from_crawler(crawler)

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider._set_lxmanager(crawler)
        return spider
