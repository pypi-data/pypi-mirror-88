
Source Code Analyzing Machine
This software attempts to analyze similarity in source code for the purpose of plagiarism detection. It supports the languages of Python and Java, but it can also analyze raw text which can be used for (although may not be as robust) other currently unsupported languages. This was built off of the revealed implementation details of the MOSS (Measure of Software Similarity) software. The document for such details can be found here:
https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf
This document explains how to use the software as well as how it works. 

How to Use It:
Running a Comparison (Basic)
Add files into the student file box by pressing the add file button at the top right. If you want to add boilerplate files, press the add boilerplate files button just below. If you see the files appear in the box, that means they’re added. You can also clear these boxes with the buttons to the right. Be sure you have the language analyzer you want to use selected; your options are C, C++, Java and Python files to analyze. When you’re ready for analysis hit compare and you will be directed to the 'View Results Tab'.

Results
To view, first add student and boilerplate files to analyze on the 'Search'. Then, from the 'View Results' tab, in the 'Results' you will see in order the file name, a visualization of its comparison with other files, the percentage matched with other files, and the number of files the document it matched with. YOu have the option to reorganized how the files are displayed, whether by percentage matched or number of documents matched, in this tab with the 'Sort By' module near the top right of results.

Detailed Document Info
To view, first add student and boilerplate files to analyze on the 'Search'. Then, from the 'View Results' tab, in the 'Results' display click the 'View Detailed Info' Button. The file's comparison information will be displayed in the 'Detailed Document Info' and a graphical representation of it's connection points in 'Visualization'.

Visualization
To view, first add student and boilerplate files to analyze on the 'Search'. Then, from the 'View Results' tab, in the 'Results' display click the 'View Detailed Info' Button. A graphical representation of the file's comparison information will be displayed in 'Visualization'. Click on a node to select then and see which file it represents.

View Fingerprints
To view fingerprints between two files, first add student or boilerplate files for analysis and click 'Compare'. From the 'View Results' tab click on the 'View Detailed Info' button to navigate file content to the 'Detailed Document Info'. For individual comparison you wish to see, click 'Send To Compare' in the ''Detailed Document Info' display and you will be navigated to the 'Individual Comparisons' tab. You will see the matching fingerprints displayed.

Build General Report
To generate a general report file, navigate to the 'Search' tab. Add the student and boilerplate files that you wish to see analyzed, then press the 'Save Results to File' on the bottom right. The output will be displayed in the output dialog and you will be given the option to save the file.

Build Individual Comparison Report
To generate an individual comparison report file, you first must add the student and boilerplate files on the 'Search' tab. Then on the 'View Results' tab. Then you will be sent to the 'Individual Comparison' tab where the indicated comparison would be shown in the 'Select Comparison' display box. Press the 'Create Report' button below this display and you will be save the individual comparison file.

Options - (Search Settings, Language, Ignore Threshold)
The language determines what analyzer will be used (C, C++, Java and Python are currently supported). The 'Ignore Threshold' determines how many allowable common fingerprints there can be before it’s put into the output.

Advanced Options - (K-Grams, Window Size)
From the 'Search' tab, click the 'Advanced' tab over setting and you will see additional options. 'K-Grams' determines what size a fingerprint should be in order for it to be truly considered a match to be factored into comparisons. The larger the k is, the more likely it is the fingerprint found was a true copy, but making it smaller makes it better able to detect reordering/repositioning of something copied and can find smaller matches. 'Window Size' determines the size of the window used in the winnowing algorithm.

Output
**DISCLAIMER**
This software is a tool and is used to help identify files which are strongly similar, but a file being similar doesn’t mean actual plagiarism took place. It can help identify suspected plagiarism, but a human is still needed to look through the files and, along with other investigation, discern whether or not copying really took place.
Output will appear below and percentage is based on relative similarity. It tells what percentage of one file’s own fingerprints are not original but similar to another file. You can index through the files and their fingerprints with the buttons below. The fingerprints being shown on the frontend aren’t actually the same as those used on the backend to decide comparisons. That uses different parameters for performance purposes, the front end uses parameters for the winnowing algorithm meant to maximize the depth of the search, sometimes it will result in more or less fingerprints. You can press the important blocks button to view blocks of consecutive fingerprints, which may be more indicative of true plagiarism (these are based off of the ones generated from the backend). If you’d like to view all blocks or prints, you can press the view all button. The clear output button clears the output field (it won’t get rid of the text file).


How it Works 
(files described here start at the path source/backend)

Parsing Source Code + Tokenization (mostly analyzer.py)

After reading in the raw file text, all whitespace is to be eliminated. This is essentially to prevent adding on a bunch of meaningless whitespace in order to attempt to throw off the algorithm, only the code itself is analyzed. All text is also brought to lowercase to prevent meaningless changing of cases that could also be used to throw off the algorithm even if the underlying text is the same. If the raw text analyzer is being used, fingerprints then begin to be developed, for source code further parsing and tokenization will also take place. Nondistinct characters (we called boilerplate characters) such as (for Python) parenthesis, colons, or ‘def’ are removed. Tokenization essentially involves taking the fundamental parts of the code and turning them into nondistinct tokens. For example, all variable names are changed to ‘v’, all function names to ‘f’, conditionals to ‘c', or loops to ‘l’. This is essentially to catch things like changing variable or function names or interchanging for loops and while loops which both do essentially the same thing in an attempt to dupe the algorithm. Comments, although they can be tokens, are usually removed altogether, as meaningless blocks of comments can be added to skew or throw off comparisons. We used the built in Python tokenizer to help tokenize Python code, and the javalang library to help tokenize Java code. A program such as this:

#this program adds 2 numbers
def sums():
var1 = 1.5
num2 = 6.3

summation1 = 0
if var1 == 1.5:
summation1 = var1 + num2

for i in range(0, 10):
num2 += i

# Display the sum
print('The sum of {0} and {1} is {2}'.format(var1, num2, summation1)
var3 = 3
num4 = 9

# Add two numbers
summation2 = var3 + num4
print('The sum of {0} and {1} is these 2 numbers: {2}'.format(var3, num4, summation2))
sums()

when parsed and tokenized may look like this:
fv=1.5v=6.3v=0cv==1.5v=v+vlv+=vf’thesumof{0}and{1}is{2}’.fv,v,vv=3v=9v=v+vf'thesumof{0}and{1}isthese2numbers:{2}'.fv,v,vf

Overall, the structure of the code itself is being most examined, rather than simply the text. This may lead to times where there seems to be a reasonably sized block of similar text between 2 files that doesn’t give a match. This doesn’t (necessarily) mean the analyzer isn’t working, but what appears to be a large block of text may only have a smaller underlying structure, at least as the analyzer sees it, that wasn’t big enough to be considered a match. There may also be times when text looks different but gives a match, this may be because things like naming or some other readily changeable property was different while underlying structure was considered similar enough to be a match.

Winnowing Algorithm and Setting Up Fingerprints (winnowing.py)

After parsing and/or tokenizing the file, the resulting string will be made into fingerprints, which are basically what are used for comparisons/analysis. First, the text is split up into k-grams, which will be a contiguous substring of length k (a changeable option). Below is an example taken from MOSS’s document:

A do run run run, a do run run
(original text)

adorunrunrunadorunrun
(irrelevant features removed/parsed)

adoru dorun orunr runru unrun nrunr runru unrun nruna runad unado nador adoru dorun orunr runru unrun
(splitting the text into k-grams with k = 5)

There should be a k-gram at every position in the text, except for the last k - 1 positions. k is also called the noise threshold, in that it will determine the length a substring should be to be considered significant enough for comparison, smaller matching substrings may simply be coincidental and aren’t necessarily indicative of plagiarism. Those would be considered a part of the general “noise” and wouldn’t be compared. Generally, the larger the k, the more sure you can be that a similar k-gram found was a true copy because of how long the match was, however a smaller k will be better at finding things that were reordered or relocated throughout the document or a smaller match, so a good value to balance that should be chosen. After being split into k-grams, the k-grams are then all hashed.

77 72 42 17 98 50 17 98 8 88 67 39 77 72 42 17 98
(a hypothetical sequence of hashes from the k-grams)

After this, the core winnowing algorithm takes place in order to derive the fingerprints, it essentially involves putting the hashes into a series of windows of size w and selecting minimum hash values. A more detailed description can be found in the document (https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf) in section 3 which goes into it better, pseudocode for it can be found in section 5.2.

17 17 8 39 17
(the hashes that were chosen as fingerprints of the original text from the winnowing algorithm)

We also keep track of the starting position of each fingerprint in the parsed text and the original unparsed substring in our fingerprint object (usually for displaying purposes). What comes out in the end would look something like this. 
runru, 17, [3]    runru, 17, [6]    nruna, 8, [8]    nador, 39, [11]    runru 17, [15]
These are essentially the fingerprints/identifiers that are chosen to make up a file and are what will be highlighted to the user on the front end. 

Comparisons (interface.py)
This takes place after all of the fingerprints in each file have been determined. Primarily, we use the compare_multiple_files function (with versions for text, Python, and Java) in order to do this, there are also single file functions that use different parameters for the winnowing algorithm that are used for the fingerprints viewable on the frontend. Files are first wrapped into filetofingerprint objects, comparisons between the objects take place, if two fingerprints between files share the same hash, then they’re considered a common fingerprint. This fingerprint can take place across multiple locations in each of the two documents, when viewing you’ll see all of the fingerprints at all of these locations across both files (essentially you’re indexing through the hashes). It should be kept in mind that a common fingerprint may share the same hash value number but the original substring they represent may not be exactly the same. 

Boilerplate Files (interface.py)
These are simply files that a student would be sanctioned to copy from. They come in the form of a parameter in the file comparing functions, and any fingerprints generated from these files are to be ignored in comparisons. One possible usage for this would be something like an answer key, or perhaps an example from a textbook or lecture slides/notes. 

Percentage Similarity (interface.py)
Percentage similarity, calculated through get_similarity, is based on the number of in-common fingerprints one file has with respect to another divided by its own total fingerprints. This means it’s a relative percentage and comparing file A to file B wouldn’t necessarily give the same percentage when comparing file B to file A, as it represents what percentage of its own fingerprints are shared by another file. 



